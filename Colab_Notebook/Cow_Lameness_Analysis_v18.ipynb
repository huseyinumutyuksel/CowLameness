{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üêÑ Cow Lameness Detection - Comprehensive Multi-Modal Pipeline v18\n",
                "\n",
                "**Architecture:**\n",
                "- YOLO v8: Multi-cow detection (select largest)\n",
                "- SAM: Segment Anything (cow isolation)\n",
                "- Pose: DeepLabCut/MMPose (configurable)\n",
                "- VideoMAE: Self-supervised visual features\n",
                "- RAFT: Optical flow (temporal motion)\n",
                "- Temporal Transformer: Multi-modal fusion\n",
                "\n",
                "**Dataset:** 1167 videos (Saƒülƒ±klƒ±: 642, Topal: 525)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install all required packages\n",
                "!pip install -q ultralytics  # YOLO v8\n",
                "!pip install -q segment-anything git+https://github.com/facebookresearch/segment-anything.git  # SAM\n",
                "!pip install -q transformers  # VideoMAE\n",
                "!pip install -q opencv-python-headless  # RAFT dependencies\n",
                "!pip install -q pandas numpy scikit-learn scipy matplotlib seaborn tqdm\n",
                "!pip install -q torch torchvision\n",
                "!pip install -q timm  # For VideoMAE\n",
                "!pip install -q pyyaml\n",
                "\n",
                "print(\"‚úÖ All packages installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import glob\n",
                "import json\n",
                "import yaml\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from ultralytics import YOLO\n",
                "from segment_anything import sam_model_registry, SamPredictor\n",
                "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"‚úÖ Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Drive & Load Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "BASE = \"/content/drive/MyDrive/Inek Topallik Tespiti Parcalanmis Inek Videolari\"\n",
                "VIDEO_DIR = f\"{BASE}/cow_single_videos\"\n",
                "POSE_CSV_DIR = f\"{BASE}/outputs/deeplabcut\"  # or mmpose based on config\n",
                "OUTPUT_DIR = f\"{BASE}/outputs/colab_results\"\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(f\"{OUTPUT_DIR}/figures\", exist_ok=True)\n",
                "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
                "\n",
                "# Load config\n",
                "config = {\n",
                "    'pose_framework': 'deeplabcut',\n",
                "    'features': {\n",
                "        'yolo_detection': True,\n",
                "        'sam_segmentation': True,\n",
                "        'videomae': True,\n",
                "        'optical_flow': True,\n",
                "        'back_curvature': True\n",
                "    }\n",
                "}\n",
                "\n",
                "print(f\"‚úÖ Config loaded: {config['pose_framework']}\")\n",
                "print(f\"   Features: {list(config['features'].keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# YOLO v8 for cow detection\n",
                "yolo_model = YOLO('yolov8n.pt')\n",
                "print(\"‚úÖ YOLO v8 loaded\")\n",
                "\n",
                "# SAM for segmentation\n",
                "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
                "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
                "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
                "sam.to(device)\n",
                "sam_predictor = SamPredictor(sam)\n",
                "print(\"‚úÖ SAM loaded\")\n",
                "\n",
                "# VideoMAE for visual features\n",
                "videomae_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
                "videomae_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
                "print(\"‚úÖ VideoMAE loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Video Processing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_largest_cow(frame, yolo_model):\n",
                "    \"\"\"Detect multiple cows, return bounding box of largest (foreground cow)\"\"\"\n",
                "    results = yolo_model(frame, classes=[19])  # Class 19 = cow in COCO\n",
                "    \n",
                "    if len(results[0].boxes) == 0:\n",
                "        return None\n",
                "    \n",
                "    # Find largest box (by area)\n",
                "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
                "    areas = [(box[2]-box[0]) * (box[3]-box[1]) for box in boxes]\n",
                "    largest_idx = np.argmax(areas)\n",
                "    \n",
                "    return boxes[largest_idx].astype(int)\n",
                "\n",
                "def segment_cow(frame, bbox, sam_predictor):\n",
                "    \"\"\"Segment cow using SAM\"\"\"\n",
                "    sam_predictor.set_image(frame)\n",
                "    \n",
                "    # Use bbox as prompt\n",
                "    masks, _, _ = sam_predictor.predict(\n",
                "        box=bbox,\n",
                "        multimask_output=False\n",
                "    )\n",
                "    \n",
                "    return masks[0]\n",
                "\n",
                "def extract_optical_flow(frames):\n",
                "    \"\"\"Compute optical flow using Farneback (lightweight alternative to RAFT)\"\"\"\n",
                "    flows = []\n",
                "    \n",
                "    for i in range(len(frames)-1):\n",
                "        gray1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
                "        gray2 = cv2.cvtColor(frames[i+1], cv2.COLOR_BGR2GRAY)\n",
                "        \n",
                "        flow = cv2.calcOpticalFlowFarneback(\n",
                "            gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
                "        )\n",
                "        \n",
                "        # Flow magnitude and angle\n",
                "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
                "        flows.append({'magnitude': mag.mean(), 'angle': ang.mean()})\n",
                "    \n",
                "    return flows\n",
                "\n",
                "def extract_videomae_features(frames, videomae_model, videomae_extractor):\n",
                "    \"\"\"Extract self-supervised visual features using VideoMAE\"\"\"\n",
                "    # Sample 16 frames (VideoMAE input)\n",
                "    indices = np.linspace(0, len(frames)-1, 16, dtype=int)\n",
                "    sampled_frames = [frames[i] for i in indices]\n",
                "    \n",
                "    # Preprocess\n",
                "    inputs = videomae_extractor(sampled_frames, return_tensors=\"pt\")\n",
                "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "    \n",
                "    # Extract features\n",
                "    with torch.no_grad():\n",
                "        outputs = videomae_model(**inputs)\n",
                "        features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
                "    \n",
                "    return features\n",
                "\n",
                "print(\"‚úÖ Processing functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Data & Extract Multi-Modal Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all videos\n",
                "video_files = []\n",
                "for label_folder in ['Saglikli', 'Topal']:\n",
                "    folder_path = f\"{VIDEO_DIR}/{label_folder}\"\n",
                "    videos = glob.glob(f\"{folder_path}/*.mp4\")\n",
                "    video_files.extend([(v, 0 if label_folder=='Saglikli' else 1) for v in videos])\n",
                "\n",
                "print(f\"üìä Found {len(video_files)} videos\")\n",
                "print(f\"   Healthy: {sum(1 for _, l in video_files if l==0)}\")\n",
                "print(f\"   Lame: {sum(1 for _, l in video_files if l==1)}\")\n",
                "\n",
                "# Process subset for demo (full processing takes hours)\n",
                "DEMO_MODE = True\n",
                "if DEMO_MODE:\n",
                "    video_files = video_files[:50]  # Process 50 videos for demo\n",
                "    print(f\"\\n‚ö†Ô∏è  DEMO MODE: Processing {len(video_files)} videos\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract multi-modal features\n",
                "dataset = []\n",
                "\n",
                "for video_path, label in tqdm(video_files, desc=\"Processing videos\"):\n",
                "    try:\n",
                "        # Load video\n",
                "        cap = cv2.VideoCapture(video_path)\n",
                "        frames = []\n",
                "        while True:\n",
                "            ret, frame = cap.read()\n",
                "            if not ret:\n",
                "                break\n",
                "            frames.append(frame)\n",
                "        cap.release()\n",
                "        \n",
                "        if len(frames) < 10:\n",
                "            continue\n",
                "        \n",
                "        # 1. YOLO: Detect largest cow\n",
                "        bbox = detect_largest_cow(frames[len(frames)//2], yolo_model)\n",
                "        \n",
                "        # 2. SAM: Segment cow\n",
                "        if bbox is not None:\n",
                "            mask = segment_cow(frames[len(frames)//2], bbox, sam_predictor)\n",
                "            mask_area = mask.sum()\n",
                "        else:\n",
                "            mask_area = 0\n",
                "        \n",
                "        # 3. Optical Flow\n",
                "        flows = extract_optical_flow(frames[::5])  # Sample every 5 frames\n",
                "        flow_mag_mean = np.mean([f['magnitude'] for f in flows])\n",
                "        flow_mag_std = np.std([f['magnitude'] for f in flows])\n",
                "        \n",
                "        # 4. VideoMAE features\n",
                "        videomae_feat = extract_videomae_features(frames, videomae_model, videomae_extractor)\n",
                "        \n",
                "        # 5. Load pose CSV\n",
                "        video_name = Path(video_path).stem\n",
                "        pose_csv = f\"{POSE_CSV_DIR}/{video_name}_DLC_SuperAnimal.csv\"\n",
                "        \n",
                "        if os.path.exists(pose_csv):\n",
                "            pose_df = pd.read_csv(pose_csv, header=[1,2])\n",
                "            pose_features = pose_df.values.mean(axis=0)[:50]  # Simplified\n",
                "        else:\n",
                "            pose_features = np.zeros(50)\n",
                "        \n",
                "        # Combine all features\n",
                "        combined_features = np.concatenate([\n",
                "            pose_features,                    # Pose (50)\n",
                "            videomae_feat[:100],              # VideoMAE (100)\n",
                "            [flow_mag_mean, flow_mag_std],    # Optical flow (2)\n",
                "            [mask_area / (640*480)]           # Segmentation (1)\n",
                "        ])\n",
                "        \n",
                "        dataset.append({\n",
                "            'video': video_name,\n",
                "            'label': label,\n",
                "            'features': combined_features\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è  Failed: {Path(video_path).name} - {e}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Processed {len(dataset)} videos\")\n",
                "print(f\"   Feature dimension: {dataset[0]['features'].shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Prepare Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = np.array([d['features'] for d in dataset])\n",
                "y = np.array([d['label'] for d in dataset])\n",
                "\n",
                "# Split\n",
                "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
                "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)\n",
                "\n",
                "# Standardize\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_val_scaled = scaler.transform(X_val)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"üìä Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Temporal Transformer Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TemporalTransformer(nn.Module):\n",
                "    \"\"\"Multi-modal Temporal Transformer for lameness classification\"\"\"\n",
                "    def __init__(self, input_dim=153, hidden_dim=256, num_heads=8, num_layers=4):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
                "        \n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=hidden_dim,\n",
                "            nhead=num_heads,\n",
                "            dim_feedforward=hidden_dim*4,\n",
                "            dropout=0.3,\n",
                "            batch_first=True\n",
                "        )\n",
                "        \n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, 2)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, features)\n",
                "        x = self.input_proj(x).unsqueeze(1)  # (batch, 1, hidden)\n",
                "        x = self.transformer(x)\n",
                "        x = x.squeeze(1)  # (batch, hidden)\n",
                "        return self.classifier(x)\n",
                "\n",
                "model = TemporalTransformer(input_dim=X_train_scaled.shape[1]).to(device)\n",
                "print(f\"‚úÖ Model created: {sum(p.numel() for p in model.parameters())} parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training with 5-Fold CV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, X_train, y_train, X_val, y_val, epochs=20):\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "    \n",
                "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
                "    y_train_t = torch.LongTensor(y_train).to(device)\n",
                "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
                "    y_val_t = torch.LongTensor(y_val).to(device)\n",
                "    \n",
                "    best_val_acc = 0\n",
                "    best_model = None\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_train_t)\n",
                "        loss = criterion(outputs, y_train_t)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            val_outputs = model(X_val_t)\n",
                "            val_preds = val_outputs.argmax(dim=1)\n",
                "            val_acc = (val_preds == y_val_t).float().mean().item()\n",
                "        \n",
                "        if val_acc > best_val_acc:\n",
                "            best_val_acc = val_acc\n",
                "            best_model = model.state_dict().copy()\n",
                "        \n",
                "        if (epoch + 1) % 5 == 0:\n",
                "            print(f\"  Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
                "    \n",
                "    return best_model, best_val_acc\n",
                "\n",
                "# 5-Fold CV\n",
                "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "fold_results = []\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):\n",
                "    print(f\"\\nFOLD {fold+1}/5\")\n",
                "    model = TemporalTransformer(input_dim=X_train_scaled.shape[1]).to(device)\n",
                "    best_model, val_acc = train_model(model, X_train_scaled[train_idx], y_train[train_idx],\n",
                "                                      X_train_scaled[val_idx], y_train[val_idx])\n",
                "    fold_results.append(val_acc)\n",
                "    print(f\"  ‚úÖ Fold {fold+1} Accuracy: {val_acc:.4f}\")\n",
                "\n",
                "print(f\"\\n5-Fold CV: {np.mean(fold_results):.4f} ¬± {np.std(fold_results):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Final Test Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final model\n",
                "final_model = TemporalTransformer(input_dim=X_train_scaled.shape[1]).to(device)\n",
                "best_model_state, _ = train_model(final_model, X_train_scaled, y_train, X_val_scaled, y_val, epochs=30)\n",
                "final_model.load_state_dict(best_model_state)\n",
                "\n",
                "# Test\n",
                "final_model.eval()\n",
                "with torch.no_grad():\n",
                "    X_test_t = torch.FloatTensor(X_test_scaled).to(device)\n",
                "    test_outputs = final_model(X_test_t)\n",
                "    test_preds = test_outputs.argmax(dim=1).cpu().numpy()\n",
                "    test_probs = torch.softmax(test_outputs, dim=1).cpu().numpy()[:, 1]\n",
                "\n",
                "# Metrics\n",
                "accuracy = accuracy_score(y_test, test_preds)\n",
                "precision, recall, f1, _ = precision_recall_fscore_support(y_test, test_preds, average='binary')\n",
                "cm = confusion_matrix(y_test, test_preds)\n",
                "auc = roc_auc_score(y_test, test_probs)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"FINAL TEST RESULTS\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"Precision: {precision:.4f}\")\n",
                "print(f\"Recall:    {recall:.4f}\")\n",
                "print(f\"F1-Score:  {f1:.4f}\")\n",
                "print(f\"ROC-AUC:   {auc:.4f}\")\n",
                "print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "# Save\n",
                "metrics = {\n",
                "    'test_accuracy': float(accuracy),\n",
                "    'precision': float(precision),\n",
                "    'recall': float(recall),\n",
                "    'f1': float(f1),\n",
                "    'roc_auc': float(auc),\n",
                "    'confusion_matrix': cm.tolist(),\n",
                "    'cv_mean_accuracy': float(np.mean(fold_results)),\n",
                "    'cv_std_accuracy': float(np.std(fold_results)),\n",
                "    'features_used': list(config['features'].keys()),\n",
                "    'timestamp': datetime.now().isoformat()\n",
                "}\n",
                "\n",
                "with open(f\"{OUTPUT_DIR}/metrics.json\", 'w') as f:\n",
                "    json.dump(metrics, f, indent=2)\n",
                "\n",
                "torch.save({\n",
                "    'model_state_dict': final_model.state_dict(),\n",
                "    'scaler': scaler,\n",
                "    'config': config\n",
                "}, f\"{OUTPUT_DIR}/models/best_model_multimodal.pth\")\n",
                "\n",
                "print(\"\\n‚úÖ Results saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Healthy', 'Lame'],\n",
                "            yticklabels=['Healthy', 'Lame'])\n",
                "plt.ylabel('True')\n",
                "plt.xlabel('Predicted')\n",
                "plt.title(f'Confusion Matrix (Acc: {accuracy:.2%})')\n",
                "plt.savefig(f\"{OUTPUT_DIR}/figures/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# ROC Curve\n",
                "fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.xlabel('FPR')\n",
                "plt.ylabel('TPR')\n",
                "plt.title('ROC Curve')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.savefig(f\"{OUTPUT_DIR}/figures/roc_curve.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}