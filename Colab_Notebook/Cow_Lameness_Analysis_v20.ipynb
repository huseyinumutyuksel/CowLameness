{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üêÑ Cow Lameness Detection - Comprehensive Multi-Modal Pipeline v18\n",
                "\n",
                "**Architecture:**\n",
                "- YOLO v8: Multi-cow detection (select largest)\n",
                "- SAM: Segment Anything (cow isolation)\n",
                "- Pose: DeepLabCut/MMPose (configurable)\n",
                "- VideoMAE: Self-supervised visual features\n",
                "- RAFT: Optical flow (temporal motion)\n",
                "- Temporal Transformer: Multi-modal fusion\n",
                "\n",
                "**Dataset:** 1167 videos (Saƒülƒ±klƒ±: 642, Topal: 525)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install all required packages\n",
                "!pip install -q ultralytics  # YOLO v8\n",
                "!pip install -q segment-anything git+https://github.com/facebookresearch/segment-anything.git  # SAM\n",
                "!pip install -q transformers  # VideoMAE\n",
                "!pip install -q opencv-python-headless  # RAFT dependencies\n",
                "!pip install -q pandas numpy scikit-learn scipy matplotlib seaborn tqdm\n",
                "!pip install -q torch torchvision\n",
                "!pip install -q timm  # For VideoMAE\n",
                "!pip install -q pyyaml\n",
                "\n",
                "print(\"‚úÖ All packages installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import glob\n",
                "import json\n",
                "import yaml\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "\n",
                "from scipy import stats\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from ultralytics import YOLO\n",
                "from segment_anything import sam_model_registry, SamPredictor\n",
                "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"‚úÖ Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Drive & Load Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "BASE = \"/content/drive/MyDrive/Inek Topallik Tespiti Parcalanmis Inek Videolari\"\n",
                "VIDEO_DIR = f\"{BASE}/cow_single_videos\"\n",
                "POSE_CSV_DIR = f\"{BASE}/outputs/deeplabcut\"  # or mmpose based on config\n",
                "OUTPUT_DIR = f\"{BASE}/outputs/colab_results\"\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(f\"{OUTPUT_DIR}/figures\", exist_ok=True)\n",
                "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
                "\n",
                "# Load config\n",
                "config = {\n",
                "    'pose_framework': 'deeplabcut',\n",
                "    'features': {\n",
                "        'yolo_detection': True,\n",
                "        'sam_segmentation': True,\n",
                "        'videomae': True,\n",
                "        'optical_flow': True,\n",
                "        'back_curvature': True\n",
                "    }\n",
                "}\n",
                "\n",
                "print(f\"‚úÖ Config loaded: {config['pose_framework']}\")\n",
                "print(f\"   Features: {list(config['features'].keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Detect Available Pose Framework"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_pose_framework(video_dir, pose_base_dir):\n",
                "    \"\"\"\n",
                "    Detect which pose estimation framework outputs are available.\n",
                "    \n",
                "    Returns:\n",
                "        tuple: (framework_name, pose_csv_dir)\n",
                "            - framework_name: 'deeplabcut', 'mmpose', or None\n",
                "            - pose_csv_dir: path to the pose CSV directory\n",
                "    \"\"\"\n",
                "    dlc_dir = f\"{pose_base_dir}/deeplabcut\"\n",
                "    mmpose_dir = f\"{pose_base_dir}/mmpose\"\n",
                "    \n",
                "    # Check for DeepLabCut outputs\n",
                "    dlc_available = False\n",
                "    dlc_files = []\n",
                "    if os.path.exists(dlc_dir):\n",
                "        dlc_files = glob.glob(f\"{dlc_dir}/*DLC*.csv\")\n",
                "        dlc_available = len(dlc_files) > 0\n",
                "    \n",
                "    # Check for MMPose outputs\n",
                "    mmpose_available = False\n",
                "    mmpose_files = []\n",
                "    if os.path.exists(mmpose_dir):\n",
                "        mmpose_files = glob.glob(f\"{mmpose_dir}/*_MMPose.csv\")\n",
                "        mmpose_available = len(mmpose_files) > 0\n",
                "    \n",
                "    # Decision logic\n",
                "    if dlc_available and mmpose_available:\n",
                "        print(\"‚ö†Ô∏è  Both DeepLabCut and MMPose outputs detected!\")\n",
                "        print(f\"   DeepLabCut: {len(dlc_files)} files\")\n",
                "        print(f\"   MMPose: {len(mmpose_files)} files\")\n",
                "        print(\"\\nWhich framework would you like to use?\")\n",
                "        print(\"  1. DeepLabCut\")\n",
                "        print(\"  2. MMPose\")\n",
                "        \n",
                "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
                "        \n",
                "        if choice == \"1\":\n",
                "            return \"deeplabcut\", dlc_dir\n",
                "        elif choice == \"2\":\n",
                "            return \"mmpose\", mmpose_dir\n",
                "        else:\n",
                "            print(\"‚ùå Invalid choice. Defaulting to DeepLabCut.\")\n",
                "            return \"deeplabcut\", dlc_dir\n",
                "    \n",
                "    elif dlc_available:\n",
                "        print(f\"‚úÖ DeepLabCut outputs detected: {len(dlc_files)} files\")\n",
                "        return \"deeplabcut\", dlc_dir\n",
                "    \n",
                "    elif mmpose_available:\n",
                "        print(f\"‚úÖ MMPose outputs detected: {len(mmpose_files)} files\")\n",
                "        return \"mmpose\", mmpose_dir\n",
                "    \n",
                "    else:\n",
                "        print(\"‚ùå ERROR: No pose estimation outputs found!\")\n",
                "        print(f\"   Checked directories:\")\n",
                "        print(f\"     - {dlc_dir}\")\n",
                "        print(f\"     - {mmpose_dir}\")\n",
                "        print(\"\\n   Please run pose estimation first:\")\n",
                "        print(\"     - DeepLabCut: python DeepLabCut/process_videos.py --batch\")\n",
                "        print(\"     - MMPose: python MMPose/process_videos.py --batch\")\n",
                "        return None, None\n",
                "\n",
                "# Detect framework\n",
                "POSE_FRAMEWORK, POSE_CSV_DIR = detect_pose_framework(VIDEO_DIR, f\"{BASE}/outputs\")\n",
                "\n",
                "if POSE_FRAMEWORK is None:\n",
                "    raise RuntimeError(\"No pose estimation data found. Cannot proceed.\")\n",
                "\n",
                "print(f\"\\nüéØ Using pose framework: {POSE_FRAMEWORK.upper()}\")\n",
                "print(f\"   CSV directory: {POSE_CSV_DIR}\")\n",
                "\n",
                "# Update config\n",
                "config['pose_framework'] = POSE_FRAMEWORK\n",
                "print(f\"\\n‚úÖ Config updated with detected framework\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# YOLO v8 for cow detection\n",
                "yolo_model = YOLO('yolov8n.pt')\n",
                "print(\"‚úÖ YOLO v8 loaded\")\n",
                "\n",
                "# SAM for segmentation\n",
                "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
                "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
                "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
                "sam.to(device)\n",
                "sam_predictor = SamPredictor(sam)\n",
                "print(\"‚úÖ SAM loaded\")\n",
                "\n",
                "# VideoMAE for visual features\n",
                "videomae_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
                "videomae_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
                "print(\"‚úÖ VideoMAE loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Video Processing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_largest_cow(frame, yolo_model):\n",
                "    \"\"\"Detect multiple cows, return bounding box of largest (foreground cow)\"\"\"\n",
                "    results = yolo_model(frame, classes=[19])  # Class 19 = cow in COCO\n",
                "    \n",
                "    if len(results[0].boxes) == 0:\n",
                "        return None\n",
                "    \n",
                "    # Find largest box (by area)\n",
                "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
                "    areas = [(box[2]-box[0]) * (box[3]-box[1]) for box in boxes]\n",
                "    largest_idx = np.argmax(areas)\n",
                "    \n",
                "    return boxes[largest_idx].astype(int)\n",
                "\n",
                "def segment_cow(frame, bbox, sam_predictor):\n",
                "    \"\"\"Segment cow using SAM\"\"\"\n",
                "    sam_predictor.set_image(frame)\n",
                "    \n",
                "    # Use bbox as prompt\n",
                "    masks, _, _ = sam_predictor.predict(\n",
                "        box=bbox,\n",
                "        multimask_output=False\n",
                "    )\n",
                "    \n",
                "    return masks[0]\n",
                "\n",
                "def extract_optical_flow(frames):\n",
                "    \"\"\"Compute optical flow using Farneback (lightweight alternative to RAFT)\"\"\"\n",
                "    flows = []\n",
                "    \n",
                "    for i in range(len(frames)-1):\n",
                "        gray1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
                "        gray2 = cv2.cvtColor(frames[i+1], cv2.COLOR_BGR2GRAY)\n",
                "        \n",
                "        flow = cv2.calcOpticalFlowFarneback(\n",
                "            gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
                "        )\n",
                "        \n",
                "        # Flow magnitude and angle\n",
                "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
                "        flows.append({'magnitude': mag.mean(), 'angle': ang.mean()})\n",
                "    \n",
                "    return flows\n",
                "\n",
                "def extract_videomae_features(frames, videomae_model, videomae_extractor):\n",
                "    \"\"\"Extract self-supervised visual features using VideoMAE\"\"\"\n",
                "    # Sample 16 frames (VideoMAE input)\n",
                "    indices = np.linspace(0, len(frames)-1, 16, dtype=int)\n",
                "    sampled_frames = [frames[i] for i in indices]\n",
                "    \n",
                "    # Preprocess\n",
                "    inputs = videomae_extractor(sampled_frames, return_tensors=\"pt\")\n",
                "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "    \n",
                "    # Extract features\n",
                "    with torch.no_grad():\n",
                "        outputs = videomae_model(**inputs)\n",
                "        features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
                "    \n",
                "    return features\n",
                "\n",
                "print(\"‚úÖ Processing functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Load Data & Extract Multi-Modal Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all videos\n",
                "video_files = []\n",
                "for label_folder in ['Saglikli', 'Topal']:\n",
                "    folder_path = f\"{VIDEO_DIR}/{label_folder}\"\n",
                "    videos = glob.glob(f\"{folder_path}/*.mp4\")\n",
                "    video_files.extend([(v, 0 if label_folder=='Saglikli' else 1) for v in videos])\n",
                "\n",
                "print(f\"üìä Found {len(video_files)} videos\")\n",
                "print(f\"   Healthy: {sum(1 for _, l in video_files if l==0)}\")\n",
                "print(f\"   Lame: {sum(1 for _, l in video_files if l==1)}\")\n",
                "\n",
                "# Process subset for demo (full processing takes hours)\n",
                "DEMO_MODE = True\n",
                "if DEMO_MODE:\n",
                "    video_files = video_files[:50]  # Process 50 videos for demo\n",
                "    print(f\"\\n‚ö†Ô∏è  DEMO MODE: Processing {len(video_files)} videos\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract multi-modal features\n",
                "dataset = []\n",
                "\n",
                "for video_path, label in tqdm(video_files, desc=\"Processing videos\"):\n",
                "    try:\n",
                "        # Load video\n",
                "        cap = cv2.VideoCapture(video_path)\n",
                "        frames = []\n",
                "        while True:\n",
                "            ret, frame = cap.read()\n",
                "            if not ret:\n",
                "                break\n",
                "            frames.append(frame)\n",
                "        cap.release()\n",
                "        \n",
                "        if len(frames) < 10:\n",
                "            continue\n",
                "        \n",
                "        # 1. YOLO: Detect largest cow\n",
                "        bbox = detect_largest_cow(frames[len(frames)//2], yolo_model)\n",
                "        \n",
                "        # 2. SAM: Segment cow\n",
                "        if bbox is not None:\n",
                "            mask = segment_cow(frames[len(frames)//2], bbox, sam_predictor)\n",
                "            mask_area = mask.sum()\n",
                "        else:\n",
                "            mask_area = 0\n",
                "        \n",
                "        # 3. Optical Flow\n",
                "        flows = extract_optical_flow(frames[::5])  # Sample every 5 frames\n",
                "        flow_mag_mean = np.mean([f['magnitude'] for f in flows])\n",
                "        flow_mag_std = np.std([f['magnitude'] for f in flows])\n",
                "        \n",
                "        # 4. VideoMAE features\n",
                "        videomae_feat = extract_videomae_features(frames, videomae_model, videomae_extractor)\n",
                "        \n",
                "        # 5. Load pose CSV\n",
                "        video_name = Path(video_path).stem\n",
                "        \n",
                "        if POSE_FRAMEWORK == \"deeplabcut\":\n",
                "            # DeepLabCut pattern: {video}DLC*.csv\n",
                "            pose_csv_pattern = f\"{POSE_CSV_DIR}/{video_name}DLC*.csv\"\n",
                "            pose_csv_files = glob.glob(pose_csv_pattern)\n",
                "            \n",
                "            if pose_csv_files:\n",
                "                pose_csv = pose_csv_files[0]  # Take first match\n",
                "                pose_df = pd.read_csv(pose_csv, header=[1,2])  # DLC has multi-level header\n",
                "                pose_features = pose_df.values.mean(axis=0)[:50]\n",
                "            else:\n",
                "                pose_features = np.zeros(50)\n",
                "                \n",
                "        elif POSE_FRAMEWORK == \"mmpose\":\n",
                "            # MMPose pattern: {video}_MMPose.csv\n",
                "            pose_csv = f\"{POSE_CSV_DIR}/{video_name}_MMPose.csv\"\n",
                "            \n",
                "            if os.path.exists(pose_csv):\n",
                "                pose_df = pd.read_csv(pose_csv, index_col=0)  # MMPose has simple header\n",
                "                pose_features = pose_df.values.mean(axis=0)[:50]\n",
                "            else:\n",
                "                pose_features = np.zeros(50)\n",
                "        \n",
                "        # Combine all features\n",
                "        combined_features = np.concatenate([\n",
                "            pose_features,                    # Pose (50)\n",
                "            videomae_feat[:100],              # VideoMAE (100)\n",
                "            [flow_mag_mean, flow_mag_std],    # Optical flow (2)\n",
                "            [mask_area / (640*480)]           # Segmentation (1)\n",
                "        ])\n",
                "        \n",
                "        dataset.append({\n",
                "            'video': video_name,\n",
                "            'label': label,\n",
                "            'features': combined_features\n",
                "        })\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è  Failed: {Path(video_path).name} - {e}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Processed {len(dataset)} videos\")\n",
                "print(f\"   Feature dimension: {dataset[0]['features'].shape[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Prepare Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Academic Standard: Train/Test split only\n",
                "# Test set is held out for final evaluation (NEVER used in training/tuning)\n",
                "X = np.array([d['features'] for d in dataset])\n",
                "y = np.array([d['label'] for d in dataset])\n",
                "\n",
                "# Split into Train (85%) and Test (15%)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.15,  # 15% for final test evaluation\n",
                "    stratify=y, \n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Standardize features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(f\"üìä Data Split (Academic Standard):\")\n",
                "print(f\"   Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
                "print(f\"   Test:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
                "print(f\"\\n   ‚úÖ Test set isolated for final evaluation\")\n",
                "print(f\"   ‚úÖ 5-Fold CV will be performed on Train set for hyperparameter tuning\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Temporal Transformer Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TemporalTransformer(nn.Module):\n",
                "    \"\"\"Multi-modal Temporal Transformer for lameness classification\"\"\"\n",
                "    def __init__(self, input_dim=153, hidden_dim=256, num_heads=8, num_layers=4):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
                "        \n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=hidden_dim,\n",
                "            nhead=num_heads,\n",
                "            dim_feedforward=hidden_dim*4,\n",
                "            dropout=0.3,\n",
                "            batch_first=True\n",
                "        )\n",
                "        \n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(128, 2)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, features)\n",
                "        x = self.input_proj(x).unsqueeze(1)  # (batch, 1, hidden)\n",
                "        x = self.transformer(x)\n",
                "        x = x.squeeze(1)  # (batch, hidden)\n",
                "        return self.classifier(x)\n",
                "\n",
                "model = TemporalTransformer(input_dim=X_train_scaled.shape[1]).to(device)\n",
                "print(f\"‚úÖ Model created: {sum(p.numel() for p in model.parameters())} parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Hyperparameter Tuning with 5-Fold CV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter configurations to test\n",
                "configs_to_test = [\n",
                "    {'hidden_dim': 256, 'num_heads': 8, 'num_layers': 4, 'dropout': 0.3},  # Default\n",
                "    {'hidden_dim': 512, 'num_heads': 8, 'num_layers': 6, 'dropout': 0.2},  # Larger\n",
                "    {'hidden_dim': 128, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.4},  # Smaller\n",
                "]\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"HYPERPARAMETER TUNING WITH 5-FOLD CROSS-VALIDATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Testing {len(configs_to_test)} configurations\")\n",
                "print(f\"Each configuration evaluated with 5-Fold CV\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Track best configuration\n",
                "best_cv_score = 0\n",
                "best_params = None\n",
                "all_results = []\n",
                "\n",
                "for config_idx, config in enumerate(configs_to_test):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Configuration {config_idx+1}/{len(configs_to_test)}\")\n",
                "    print(f\"  hidden_dim={config['hidden_dim']}, num_heads={config['num_heads']}\")\n",
                "    print(f\"  num_layers={config['num_layers']}, dropout={config['dropout']}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # 5-Fold CV for this configuration\n",
                "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "    fold_results = []\n",
                "    \n",
                "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):\n",
                "        print(f\"  Fold {fold+1}/5...\", end=' ')\n",
                "        \n",
                "        # Create model with current config\n",
                "        model = TemporalTransformer(\n",
                "            input_dim=X_train_scaled.shape[1],\n",
                "            hidden_dim=config['hidden_dim'],\n",
                "            num_heads=config['num_heads'],\n",
                "            num_layers=config['num_layers']\n",
                "        ).to(device)\n",
                "        \n",
                "        # Train on this fold\n",
                "        best_model, val_acc = train_model(\n",
                "            model, \n",
                "            X_train_scaled[train_idx], y_train[train_idx],\n",
                "            X_train_scaled[val_idx], y_train[val_idx],\n",
                "            epochs=20\n",
                "        )\n",
                "        \n",
                "        fold_results.append(val_acc)\n",
                "        print(f\"Accuracy: {val_acc:.4f}\")\n",
                "    \n",
                "    # Calculate mean CV score\n",
                "    mean_cv_score = np.mean(fold_results)\n",
                "    std_cv_score = np.std(fold_results)\n",
                "    \n",
                "    print(f\"\\n  üìä Configuration {config_idx+1} Results:\")\n",
                "    print(f\"     Mean CV Accuracy: {mean_cv_score:.4f} ¬± {std_cv_score:.4f}\")\n",
                "    print(f\"     Fold Accuracies: {[f'{acc:.4f}' for acc in fold_results]}\")\n",
                "    \n",
                "    all_results.append({\n",
                "        'config': config,\n",
                "        'mean_cv_score': mean_cv_score,\n",
                "        'std_cv_score': std_cv_score,\n",
                "        'fold_results': fold_results\n",
                "    })\n",
                "    \n",
                "    # Track best configuration\n",
                "    if mean_cv_score > best_cv_score:\n",
                "        best_cv_score = mean_cv_score\n",
                "        best_params = config\n",
                "        print(f\"     ‚úÖ New best configuration!\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Best Configuration:\")\n",
                "print(f\"  {best_params}\")\n",
                "print(f\"  CV Accuracy: {best_cv_score:.4f}\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Final Model Training & Test Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"FINAL MODEL TRAINING\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Training with best hyperparameters on FULL training set\")\n",
                "print(f\"  Parameters: {best_params}\")\n",
                "print(f\"  Training samples: {len(X_train_scaled)}\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "# Create final model with best hyperparameters\n",
                "final_model = TemporalTransformer(\n",
                "    input_dim=X_train_scaled.shape[1],\n",
                "    hidden_dim=best_params['hidden_dim'],\n",
                "    num_heads=best_params['num_heads'],\n",
                "    num_layers=best_params['num_layers']\n",
                ").to(device)\n",
                "\n",
                "# Train on FULL training set (no validation split)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
                "\n",
                "X_train_t = torch.FloatTensor(X_train_scaled).to(device)\n",
                "y_train_t = torch.LongTensor(y_train).to(device)\n",
                "\n",
                "for epoch in range(30):\n",
                "    final_model.train()\n",
                "    optimizer.zero_grad()\n",
                "    outputs = final_model(X_train_t)\n",
                "    loss = criterion(outputs, y_train_t)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f\"  Epoch {epoch+1}/30 - Loss: {loss.item():.4f}\")\n",
                "\n",
                "print(\"\\n‚úÖ Final model training complete\")\n",
                "\n",
                "# Evaluate on held-out test set\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EVALUATING ON HELD-OUT TEST SET\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "final_model.eval()\n",
                "with torch.no_grad():\n",
                "    X_test_t = torch.FloatTensor(X_test_scaled).to(device)\n",
                "    test_outputs = final_model(X_test_t)\n",
                "    test_preds = test_outputs.argmax(dim=1).cpu().numpy()\n",
                "    test_probs = torch.softmax(test_outputs, dim=1).cpu().numpy()[:, 1]\n",
                "\n",
                "# Calculate metrics\n",
                "accuracy = accuracy_score(y_test, test_preds)\n",
                "precision, recall, f1, _ = precision_recall_fscore_support(y_test, test_preds, average='binary')\n",
                "cm = confusion_matrix(y_test, test_preds)\n",
                "auc = roc_auc_score(y_test, test_probs)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"FINAL TEST RESULTS\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"Precision:      {precision:.4f}\")\n",
                "print(f\"Recall:         {recall:.4f}\")\n",
                "print(f\"F1-Score:       {f1:.4f}\")\n",
                "print(f\"ROC-AUC:        {auc:.4f}\")\n",
                "print(f\"\\nConfusion Matrix:\")\n",
                "print(cm)\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "# Academic reporting\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"ACADEMIC SUMMARY\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Cross-Validation (Training Set):\")\n",
                "print(f\"  Mean Accuracy: {best_cv_score:.4f}\")\n",
                "print(f\"\\nFinal Test Set Performance:\")\n",
                "print(f\"  Accuracy: {accuracy:.4f}\")\n",
                "print(f\"  F1-Score: {f1:.4f}\")\n",
                "print(f\"  ROC-AUC:  {auc:.4f}\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "# Save comprehensive results\n",
                "metrics = {\n",
                "    'methodology': '5-Fold Cross-Validation for Hyperparameter Tuning',\n",
                "    'data_split': {\n",
                "        'train_size': len(X_train),\n",
                "        'test_size': len(X_test),\n",
                "        'train_percentage': 85,\n",
                "        'test_percentage': 15\n",
                "    },\n",
                "    'best_hyperparameters': best_params,\n",
                "    'cv_results': {\n",
                "        'mean_accuracy': float(best_cv_score),\n",
                "        'all_configurations': all_results\n",
                "    },\n",
                "    'test_results': {\n",
                "        'accuracy': float(accuracy),\n",
                "        'precision': float(precision),\n",
                "        'recall': float(recall),\n",
                "        'f1_score': float(f1),\n",
                "        'roc_auc': float(auc),\n",
                "        'confusion_matrix': cm.tolist()\n",
                "    },\n",
                "    'features_used': list(config['features'].keys()),\n",
                "    'timestamp': datetime.now().isoformat()\n",
                "}\n",
                "\n",
                "with open(f\"{OUTPUT_DIR}/metrics.json\", 'w') as f:\n",
                "    json.dump(metrics, f, indent=2)\n",
                "\n",
                "torch.save({\n",
                "    'model_state_dict': final_model.state_dict(),\n",
                "    'scaler': scaler,\n",
                "    'config': config,\n",
                "    'best_params': best_params,\n",
                "    'cv_score': best_cv_score,\n",
                "    'test_metrics': metrics['test_results']\n",
                "}, f\"{OUTPUT_DIR}/models/best_model_multimodal.pth\")\n",
                "\n",
                "print(\"\\n‚úÖ Results saved to:\")\n",
                "print(f\"   - {OUTPUT_DIR}/metrics.json\")\n",
                "print(f\"   - {OUTPUT_DIR}/models/best_model_multimodal.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Healthy', 'Lame'],\n",
                "            yticklabels=['Healthy', 'Lame'])\n",
                "plt.ylabel('True')\n",
                "plt.xlabel('Predicted')\n",
                "plt.title(f'Confusion Matrix (Acc: {accuracy:.2%})')\n",
                "plt.savefig(f\"{OUTPUT_DIR}/figures/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# ROC Curve\n",
                "fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
                "plt.plot([0, 1], [0, 1], 'k--')\n",
                "plt.xlabel('FPR')\n",
                "plt.ylabel('TPR')\n",
                "plt.title('ROC Curve')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.savefig(f\"{OUTPUT_DIR}/figures/roc_curve.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úÖ Complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}