{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ„ Cow Lameness Detection - Gold Standard Pipeline v21\n",
                "\n",
                "## Gait-Based Multi-Modal Temporal Analysis\n",
                "\n",
                "**Architecture:**\n",
                "- âœ… **Pose**: Biomechanical features (symmetry, velocity, acceleration)\n",
                "- âœ… **Optical Flow**: Motion irregularity analysis\n",
                "- âœ… **VideoMAE**: Partial fine-tuning (blocks 9-12 trainable)\n",
                "- âœ… **Transformer MIL**: Weak-label video learning\n",
                "- âœ… **Causal Attention**: Online prediction ready\n",
                "\n",
                "**Key Features:**\n",
                "- Sliding window (16-32 frames, 50% overlap)\n",
                "- Multiple Instance Learning for video-level labels\n",
                "- Interpretable temporal attention\n",
                "- Severity regression (0-3 score)\n",
                "\n",
                "---\n",
                "\n",
                "**References:**\n",
                "- Flower et al., 2008: Temporal asymmetry >10% â†’ 80% lameness detection\n",
                "- Van Nuffel et al., 2015: Hip angle variance â†’ 75% accuracy\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 1.1 INSTALL DEPENDENCIES\n",
                "# ============================================================\n",
                "\n",
                "!pip install -q transformers timm einops opencv-python-headless torchmetrics\n",
                "!pip install -q pandas numpy scipy scikit-learn matplotlib seaborn tqdm\n",
                "\n",
                "print(\"âœ… Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 1.2 IMPORTS\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "import json\n",
                "import random\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from scipy.signal import find_peaks\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from einops import rearrange\n",
                "import timm\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"âœ… All imports successful\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 1.3 REPRODUCIBILITY (GOLD STANDARD REQUIREMENT)\n",
                "# ============================================================\n",
                "\n",
                "SEED = 42\n",
                "\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "torch.cuda.manual_seed_all(SEED)\n",
                "\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"âœ… Reproducibility configured\")\n",
                "print(f\"   Seed: {SEED}\")\n",
                "print(f\"   Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount Drive & Hard-Coded Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 2.1 MOUNT GOOGLE DRIVE\n",
                "# ============================================================\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print(\"âœ… Google Drive mounted\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 2.2 HARD-CODED PATHS (FROM YOUR v20 - DO NOT CHANGE)\n",
                "# ============================================================\n",
                "\n",
                "# Raw videos (USER ORIGINAL PATH)\n",
                "VIDEO_DIR = \"/content/drive/MyDrive/Inek Topallik Tespiti Parcalanmis Inek Videolari/cow_single_videos\"\n",
                "\n",
                "# Pose estimation outputs (DeepLabCut)\n",
                "POSE_DIR = \"/content/drive/MyDrive/DeepLabCut/outputs\"\n",
                "\n",
                "# Derived data (local runtime - don't bloat Drive)\n",
                "FLOW_DIR = \"/content/optical_flow\"\n",
                "VIDEOMAE_DIR = \"/content/videomae_features\"\n",
                "MODEL_DIR = \"/content/models\"\n",
                "RESULT_DIR = \"/content/results\"\n",
                "\n",
                "# Create directories\n",
                "for d in [FLOW_DIR, VIDEOMAE_DIR, MODEL_DIR, RESULT_DIR]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "print(\"âœ… Paths configured\")\n",
                "print(f\"   VIDEO_DIR: {VIDEO_DIR}\")\n",
                "print(f\"   POSE_DIR: {POSE_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 2.3 VERIFY DATA EXISTS\n",
                "# ============================================================\n",
                "\n",
                "# Check for videos\n",
                "healthy_videos = glob(f\"{VIDEO_DIR}/Saglikli/*.mp4\")\n",
                "lame_videos = glob(f\"{VIDEO_DIR}/Topal/*.mp4\")\n",
                "\n",
                "print(f\"ðŸ“Š Dataset Overview:\")\n",
                "print(f\"   Healthy videos: {len(healthy_videos)}\")\n",
                "print(f\"   Lame videos: {len(lame_videos)}\")\n",
                "print(f\"   Total: {len(healthy_videos) + len(lame_videos)}\")\n",
                "\n",
                "# Check for pose CSVs\n",
                "pose_files_healthy = glob(f\"{POSE_DIR}/Saglikli/*DLC*.csv\")\n",
                "pose_files_lame = glob(f\"{POSE_DIR}/Topal/*DLC*.csv\")\n",
                "\n",
                "print(f\"\\nðŸ“Š Pose Data:\")\n",
                "print(f\"   Healthy CSVs: {len(pose_files_healthy)}\")\n",
                "print(f\"   Lame CSVs: {len(pose_files_lame)}\")\n",
                "\n",
                "if len(healthy_videos) == 0 or len(lame_videos) == 0:\n",
                "    print(\"\\nâš ï¸  WARNING: No videos found! Check VIDEO_DIR path.\")\n",
                "else:\n",
                "    print(\"\\nâœ… Data verification complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Global Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\\n",
                "# 3.1 GLOBAL CONFIGURATION (GOLD STANDARD v21+)\\n",
                "# ============================================================\\n",
                "\\n",
                "CFG = {\\n",
                "    # Video processing\\n",
                "    \\\"FPS\\\": 30,\\n",
                "    \\\"WINDOW_FRAMES\\\": 60,\\n",
                "    \\\"STRIDE_FRAMES\\\": 15,\\n",
                "    \\n",
                "    # Model dimensions\\n",
                "    \\\"POSE_DIM\\\": 16,\\n",
                "    \\\"FLOW_DIM\\\": 3,\\n",
                "    \\\"VIDEO_DIM\\\": 128,\\n",
                "    \\\"HIDDEN_DIM\\\": 256,\\n",
                "    \\\"NUM_HEADS\\\": 8,\\n",
                "    \\\"NUM_LAYERS\\\": 4,\\n",
                "    \\n",
                "    # Training\\n",
                "    \\\"BATCH_SIZE\\\": 1,\\n",
                "    \\\"EPOCHS\\\": 30,\\n",
                "    \\\"LR\\\": 1e-4,\\n",
                "    \\\"LR_VIDEOMAE\\\": 1e-5,\\n",
                "    \\\"WEIGHT_DECAY\\\": 1e-4,\\n",
                "    \\n",
                "    # Ablation config\\n",
                "    \\\"USE_POSE\\\": True,\\n",
                "    \\\"USE_FLOW\\\": True,\\n",
                "    \\\"USE_VIDEOMAE\\\": True,  # âœ… ENABLED\\n",
                "    \\n",
                "    # Gold Standard features\\n",
                "    \\\"MODE\\\": \\\"regression\\\",\\n",
                "    \\\"USE_CAUSAL\\\": True,\\n",
                "    \\\"USE_SSL\\\": False,\\n",
                "    \\\"TRAINABLE_BLOCKS\\\": [9, 10, 11],\\n",
                "}\\n",
                "\\n",
                "print(\\\"âœ… Gold Standard Configuration\\\")\\n",
                "for k, v in CFG.items():\\n",
                "    print(f\\\"   {k}: {v}\\\")\\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Biomechanical Pose Feature Extraction\n",
                "\n",
                "**This is the PRIMARY signal of the system.**\n",
                "\n",
                "Key features:\n",
                "- Temporal asymmetry (left-right step difference) - **MOST IMPORTANT**\n",
                "- Step duration statistics\n",
                "- Joint angle analysis\n",
                "- Hip sway amplitude\n",
                "\n",
                "Reference: Flower et al., 2008 - Temporal asymmetry >10% â†’ 80% sensitivity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 4.1 GAIT FEATURE EXTRACTOR (BIOMECHANICAL CORE)\n",
                "# ============================================================\n",
                "\n",
                "class GaitFeatureExtractor:\n",
                "    \"\"\"\n",
                "    Extract biomechanical gait features from pose keypoints.\n",
                "    This is the PRIMARY signal for lameness detection.\n",
                "    \n",
                "    Features:\n",
                "    - Temporal asymmetry (left-right step difference)\n",
                "    - Step duration statistics\n",
                "    - Joint angle analysis\n",
                "    - Hip sway amplitude\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, fps: float = 30.0):\n",
                "        self.fps = fps\n",
                "    \n",
                "    def extract_features(self, pose_df: pd.DataFrame) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        Extract gait features from pose DataFrame.\n",
                "        \n",
                "        Returns:\n",
                "            Feature array of shape (T, F) where F = 16 features\n",
                "        \"\"\"\n",
                "        try:\n",
                "            # Get raw coordinates\n",
                "            coords = pose_df.values\n",
                "            T = len(coords)\n",
                "            \n",
                "            if T < 30:\n",
                "                return None\n",
                "            \n",
                "            # Reshape to (T, num_keypoints, 3) - x, y, confidence\n",
                "            num_cols = coords.shape[1]\n",
                "            num_kp = num_cols // 3\n",
                "            kp = coords.reshape(T, num_kp, 3)\n",
                "            \n",
                "            features = []\n",
                "            \n",
                "            for t in range(T):\n",
                "                frame_feats = []\n",
                "                \n",
                "                # 1. Velocity features (kinematics)\n",
                "                if t > 0:\n",
                "                    vel = np.linalg.norm(kp[t, :, :2] - kp[t-1, :, :2], axis=1)\n",
                "                    frame_feats.extend([\n",
                "                        np.mean(vel),      # Mean velocity\n",
                "                        np.std(vel),       # Velocity variance\n",
                "                        np.max(vel),       # Max velocity\n",
                "                    ])\n",
                "                else:\n",
                "                    frame_feats.extend([0, 0, 0])\n",
                "                \n",
                "                # 2. Acceleration features\n",
                "                if t > 1:\n",
                "                    vel_curr = np.linalg.norm(kp[t, :, :2] - kp[t-1, :, :2], axis=1)\n",
                "                    vel_prev = np.linalg.norm(kp[t-1, :, :2] - kp[t-2, :, :2], axis=1)\n",
                "                    acc = np.abs(vel_curr - vel_prev)\n",
                "                    frame_feats.extend([\n",
                "                        np.mean(acc),      # Mean acceleration\n",
                "                        np.std(acc),       # Acceleration variance\n",
                "                    ])\n",
                "                else:\n",
                "                    frame_feats.extend([0, 0])\n",
                "                \n",
                "                # 3. Symmetry features (MOST IMPORTANT FOR LAMENESS)\n",
                "                mid = num_kp // 2\n",
                "                left_side = kp[t, :mid, :2]\n",
                "                right_side = kp[t, mid:, :2]\n",
                "                \n",
                "                if left_side.shape == right_side.shape:\n",
                "                    sym_diff = np.abs(left_side - right_side).mean()\n",
                "                    frame_feats.append(sym_diff)  # Asymmetry\n",
                "                else:\n",
                "                    frame_feats.append(0)\n",
                "                \n",
                "                # 4. Vertical oscillation (gait stability)\n",
                "                y_coords = kp[t, :, 1]\n",
                "                frame_feats.extend([\n",
                "                    np.std(y_coords),      # Vertical stability\n",
                "                    np.ptp(y_coords),      # Vertical range\n",
                "                ])\n",
                "                \n",
                "                # 5. Hip sway (horizontal displacement)\n",
                "                x_coords = kp[t, :, 0]\n",
                "                frame_feats.extend([\n",
                "                    np.std(x_coords),      # Horizontal sway\n",
                "                    np.ptp(x_coords),      # Horizontal range\n",
                "                ])\n",
                "                \n",
                "                # 6. Confidence-weighted features\n",
                "                conf = kp[t, :, 2]\n",
                "                frame_feats.extend([\n",
                "                    np.mean(conf),         # Mean confidence\n",
                "                    np.min(conf),          # Min confidence (detection quality)\n",
                "                ])\n",
                "                \n",
                "                # 7. Joint angles (if enough keypoints)\n",
                "                if num_kp >= 4:\n",
                "                    # Simple angle calculation between adjacent keypoints\n",
                "                    angles = []\n",
                "                    for i in range(num_kp - 2):\n",
                "                        v1 = kp[t, i+1, :2] - kp[t, i, :2]\n",
                "                        v2 = kp[t, i+2, :2] - kp[t, i+1, :2]\n",
                "                        cos_ang = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)\n",
                "                        angle = np.arccos(np.clip(cos_ang, -1, 1))\n",
                "                        angles.append(angle)\n",
                "                    frame_feats.extend([\n",
                "                        np.mean(angles),   # Mean joint angle\n",
                "                        np.std(angles),    # Angle variance\n",
                "                    ])\n",
                "                else:\n",
                "                    frame_feats.extend([0, 0])\n",
                "                \n",
                "                # Pad to fixed size (16 features)\n",
                "                while len(frame_feats) < 16:\n",
                "                    frame_feats.append(0)\n",
                "                frame_feats = frame_feats[:16]\n",
                "                \n",
                "                features.append(frame_feats)\n",
                "            \n",
                "            return np.array(features)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error extracting features: {e}\")\n",
                "            return None\n",
                "\n",
                "\n",
                "# Initialize extractor\n",
                "gait_extractor = GaitFeatureExtractor(fps=CFG[\"FPS\"])\n",
                "\n",
                "print(\"âœ… GaitFeatureExtractor initialized\")\n",
                "print(f\"   Output features: {CFG['POSE_DIM']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 4.2 TEST POSE FEATURE EXTRACTION\n",
                "# ============================================================\n",
                "\n",
                "# Find a sample pose CSV\n",
                "sample_pose_files = glob(f\"{POSE_DIR}/Saglikli/*DLC*.csv\")[:1]\n",
                "\n",
                "if sample_pose_files:\n",
                "    sample_csv = sample_pose_files[0]\n",
                "    print(f\"ðŸ“Š Testing with: {Path(sample_csv).name}\")\n",
                "    \n",
                "    # Load pose CSV (DeepLabCut format with multi-level header)\n",
                "    try:\n",
                "        pose_df = pd.read_csv(sample_csv, header=[1, 2])\n",
                "        print(f\"   Shape: {pose_df.shape}\")\n",
                "        print(f\"   Columns (first 5): {list(pose_df.columns[:5])}\")\n",
                "        \n",
                "        # Extract features\n",
                "        features = gait_extractor.extract_features(pose_df)\n",
                "        \n",
                "        if features is not None:\n",
                "            print(f\"\\nâœ… Feature extraction successful!\")\n",
                "            print(f\"   Output shape: {features.shape}\")\n",
                "            print(f\"   Feature range: [{features.min():.4f}, {features.max():.4f}]\")\n",
                "        else:\n",
                "            print(\"\\nâš ï¸  Feature extraction returned None\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"\\nâŒ Error: {e}\")\n",
                "else:\n",
                "    print(\"âš ï¸  No pose CSV files found for testing\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optical Flow Feature Extraction\n",
                "\n",
                "Flow provides **appearance-agnostic motion** information.\n",
                "\n",
                "Features:\n",
                "- Magnitude mean (motion intensity)\n",
                "- Magnitude variance (motion consistency)\n",
                "- Direction entropy (motion irregularity)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.1 OPTICAL FLOW EXTRACTOR\n",
                "# ============================================================\n",
                "\n",
                "def extract_optical_flow_features(video_path: str, window: int = 16, stride: int = 8) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Extract optical flow features from video.\n",
                "    \n",
                "    Returns:\n",
                "        Feature array of shape (N_windows, 3):\n",
                "        - magnitude_mean\n",
                "        - magnitude_var\n",
                "        - direction_entropy\n",
                "    \"\"\"\n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    frames = []\n",
                "    \n",
                "    while True:\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        frames.append(gray)\n",
                "    \n",
                "    cap.release()\n",
                "    \n",
                "    if len(frames) < window:\n",
                "        return None\n",
                "    \n",
                "    # Sliding windows\n",
                "    flow_feats = []\n",
                "    \n",
                "    for start in range(0, len(frames) - window + 1, stride):\n",
                "        window_frames = frames[start:start + window]\n",
                "        \n",
                "        mags = []\n",
                "        angs = []\n",
                "        prev = window_frames[0]\n",
                "        \n",
                "        for f in window_frames[1:]:\n",
                "            flow = cv2.calcOpticalFlowFarneback(\n",
                "                prev, f, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
                "            )\n",
                "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
                "            mags.append(mag.mean())\n",
                "            angs.append(ang.std())  # Direction entropy\n",
                "            prev = f\n",
                "        \n",
                "        flow_feats.append([\n",
                "            np.mean(mags),           # Magnitude mean\n",
                "            np.var(mags),            # Magnitude variance\n",
                "            np.mean(angs),           # Direction entropy\n",
                "        ])\n",
                "    \n",
                "    return np.array(flow_feats)\n",
                "\n",
                "\n",
                "print(\"âœ… Optical flow extractor defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.2 TEST OPTICAL FLOW EXTRACTION\n",
                "# ============================================================\n",
                "\n",
                "sample_videos = glob(f\"{VIDEO_DIR}/Saglikli/*.mp4\")[:1]\n",
                "\n",
                "if sample_videos:\n",
                "    sample_video = sample_videos[0]\n",
                "    print(f\"ðŸ“Š Testing with: {Path(sample_video).name}\")\n",
                "    \n",
                "    flow_features = extract_optical_flow_features(sample_video)\n",
                "    \n",
                "    if flow_features is not None:\n",
                "        print(f\"\\nâœ… Flow extraction successful!\")\n",
                "        print(f\"   Output shape: {flow_features.shape}\")\n",
                "        print(f\"   Mag mean range: [{flow_features[:, 0].min():.4f}, {flow_features[:, 0].max():.4f}]\")\n",
                "    else:\n",
                "        print(\"\\nâš ï¸  Flow extraction returned None (video too short)\")\n",
                "else:\n",
                "    print(\"âš ï¸  No video files found for testing\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.5 VIDEOMAE ENCODER (v25 GOLD STANDARD REQUIREMENT)\n",
                "# ============================================================\n",
                "\n",
                "from transformers import VideoMAEModel\n",
                "\n",
                "class VideoMAEEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    VideoMAE encoder with partial fine-tuning strategy.\n",
                "    \n",
                "    Architecture:\n",
                "    - Blocks 0-8: FROZEN (preserve general motion representation)\n",
                "    - Blocks 9-11: TRAINABLE (adapt to lameness-specific patterns)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model_name='MCG-NJU/videomae-base', \n",
                "                 output_dim=128, trainable_blocks=[9, 10, 11]):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.videomae = VideoMAEModel.from_pretrained(\n",
                "            model_name, output_hidden_states=True\n",
                "        )\n",
                "        \n",
                "        # Partial fine-tuning: freeze early layers\n",
                "        for param in self.videomae.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        for name, param in self.videomae.named_parameters():\n",
                "            for block_idx in trainable_blocks:\n",
                "                if f'encoder.layer.{block_idx}' in name:\n",
                "                    param.requires_grad = True\n",
                "                    break\n",
                "        \n",
                "        # Projection layer\n",
                "        self.projection = nn.Sequential(\n",
                "            nn.Linear(768, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.1),\n",
                "            nn.Linear(256, output_dim),\n",
                "            nn.LayerNorm(output_dim)\n",
                "        )\n",
                "        \n",
                "        # Count trainable params\n",
                "        trainable = sum(p.numel() for p in self.videomae.parameters() if p.requires_grad)\n",
                "        total = sum(p.numel() for p in self.videomae.parameters())\n",
                "        print(f'VideoMAE: {trainable:,}/{total:,} params trainable ({100*trainable/total:.1f}%)')\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (B, T, C, H, W)\n",
                "        B, T, C, H, W = x.shape\n",
                "        x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
                "        \n",
                "        outputs = self.videomae(pixel_values=x)\n",
                "        hidden = outputs.last_hidden_state  # (B, num_patches, 768)\n",
                "        \n",
                "        # Mean pooling over patches\n",
                "        video_feat = hidden.mean(dim=1)  # (B, 768)\n",
                "        \n",
                "        return self.projection(video_feat)  # (B, output_dim)\n",
                "\n",
                "\n",
                "# Initialize VideoMAE if enabled\n",
                "videomae_encoder = None\n",
                "if CFG['USE_VIDEOMAE']:\n",
                "    videomae_encoder = VideoMAEEncoder(\n",
                "        output_dim=CFG['VIDEO_DIM'],\n",
                "        trainable_blocks=CFG.get('TRAINABLE_BLOCKS', [9, 10, 11])\n",
                "    ).to(DEVICE)\n",
                "    print('âœ… VideoMAE Encoder initialized with partial fine-tuning')\n",
                "else:\n",
                "    print('âš ï¸ VideoMAE disabled in config')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.6 DOMAIN NORMALIZATION (v29 REQUIREMENT)\n",
                "# ============================================================\n",
                "\n",
                "class DomainNorm(nn.Module):\n",
                "    \"\"\"\n",
                "    Domain Normalization for cross-farm generalization.\n",
                "    Handles domain shift between different farms/cameras.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, dim, eps=1e-6):\n",
                "        super().__init__()\n",
                "        self.layer_norm = nn.LayerNorm(dim, eps=eps)\n",
                "        self.scale = nn.Parameter(torch.ones(dim))\n",
                "        self.shift = nn.Parameter(torch.zeros(dim))\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.layer_norm(x)\n",
                "        return x * self.scale + self.shift\n",
                "\n",
                "print('âœ… DomainNorm defined')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.7 CAUSAL TRANSFORMER (v30 REQUIREMENT)\n",
                "# ============================================================\n",
                "\n",
                "class CausalTransformerEncoder(nn.Module):\n",
                "    \"\"\"\n",
                "    Causal Transformer Encoder.\n",
                "    Uses causal mask to prevent information leakage from future.\n",
                "    Enables online/streaming prediction.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, d_model=256, nhead=8, num_layers=4, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        \n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model,\n",
                "            nhead=nhead,\n",
                "            dim_feedforward=d_model * 4,\n",
                "            dropout=dropout,\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        self._causal_mask = None\n",
                "    \n",
                "    def _get_causal_mask(self, seq_len, device):\n",
                "        if self._causal_mask is None or self._causal_mask.size(0) != seq_len:\n",
                "            # Upper triangular mask (prevents attending to future)\n",
                "            self._causal_mask = torch.triu(\n",
                "                torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
                "            ).bool()\n",
                "        return self._causal_mask\n",
                "    \n",
                "    def forward(self, x, use_causal=True):\n",
                "        B, T, D = x.shape\n",
                "        \n",
                "        if use_causal:\n",
                "            mask = self._get_causal_mask(T, x.device)\n",
                "        else:\n",
                "            mask = None\n",
                "        \n",
                "        return self.transformer(x, mask=mask)\n",
                "\n",
                "print('âœ… CausalTransformerEncoder defined')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 5.8 SELF-SUPERVISED PRETRAINING (v29 REQUIREMENT)\n",
                "# ============================================================\n",
                "\n",
                "class TemporalOrderNet(nn.Module):\n",
                "    \"\"\"\n",
                "    Self-Supervised Temporal Order Verification.\n",
                "    Pretext task: Predict if frame order is correct or reversed.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, input_dim=768, hidden_dim=256):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.encoder = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.1),\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        self.classifier = nn.Linear(hidden_dim, 2)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (T, D)\n",
                "        encoded = self.encoder(x)  # (T, hidden)\n",
                "        pooled = encoded.mean(dim=0)  # (hidden,)\n",
                "        return self.classifier(pooled).unsqueeze(0)\n",
                "\n",
                "print('âœ… TemporalOrderNet defined (SSL)')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Sliding Window Generator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 6.1 SLIDING WINDOW GENERATOR\n",
                "# ============================================================\n",
                "\n",
                "def generate_sliding_windows(features: np.ndarray, \n",
                "                              window_size: int, \n",
                "                              stride: int) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Generate overlapping temporal windows from feature sequence.\n",
                "    \n",
                "    Args:\n",
                "        features: Array of shape (T, F)\n",
                "        window_size: Number of frames per window\n",
                "        stride: Step between windows\n",
                "    \n",
                "    Returns:\n",
                "        Array of shape (N_windows, window_size, F)\n",
                "    \"\"\"\n",
                "    if features is None or len(features) < window_size:\n",
                "        return None\n",
                "    \n",
                "    windows = []\n",
                "    for start in range(0, len(features) - window_size + 1, stride):\n",
                "        windows.append(features[start:start + window_size])\n",
                "    \n",
                "    if len(windows) == 0:\n",
                "        return None\n",
                "    \n",
                "    return np.stack(windows)\n",
                "\n",
                "\n",
                "print(\"âœ… Sliding window generator defined\")\n",
                "print(f\"   Window size: {CFG['WINDOW_FRAMES']} frames\")\n",
                "print(f\"   Stride: {CFG['STRIDE_FRAMES']} frames\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Dataset Class (MIL-Compatible)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 7.1 COW LAMENESS DATASET (ABLATION-AWARE)\n",
                "# ============================================================\n",
                "\n",
                "class CowLamenessDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset for cow lameness detection with MIL support.\n",
                "    \n",
                "    Each sample is a video represented as multiple temporal windows.\n",
                "    Label is video-level (weak supervision).\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, video_list, labels, config, pose_dir, \n",
                "                 window_size=60, stride=15):\n",
                "        self.video_list = video_list\n",
                "        self.labels = labels\n",
                "        self.config = config\n",
                "        self.pose_dir = pose_dir\n",
                "        self.window_size = window_size\n",
                "        self.stride = stride\n",
                "        self.gait_extractor = GaitFeatureExtractor(fps=30.0)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.video_list)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        video_path = self.video_list[idx]\n",
                "        label = self.labels[idx]\n",
                "        video_name = Path(video_path).stem\n",
                "        \n",
                "        pose_feat = None\n",
                "        flow_feat = None\n",
                "        \n",
                "        # Extract pose features\n",
                "        if self.config[\"USE_POSE\"]:\n",
                "            # Find corresponding pose CSV\n",
                "            sub_folder = \"Saglikli\" if label == 0 else \"Topal\"\n",
                "            pose_pattern = f\"{self.pose_dir}/{sub_folder}/{video_name}DLC*.csv\"\n",
                "            pose_files = glob(pose_pattern)\n",
                "            \n",
                "            if pose_files:\n",
                "                try:\n",
                "                    pose_df = pd.read_csv(pose_files[0], header=[1, 2])\n",
                "                    pose_feat = self.gait_extractor.extract_features(pose_df)\n",
                "                    if pose_feat is not None:\n",
                "                        pose_feat = generate_sliding_windows(\n",
                "                            pose_feat, self.window_size, self.stride\n",
                "                        )\n",
                "                except Exception as e:\n",
                "                    pass\n",
                "        \n",
                "        # Extract flow features\n",
                "        if self.config[\"USE_FLOW\"]:\n",
                "            flow_feat = extract_optical_flow_features(\n",
                "                video_path, window=16, stride=8\n",
                "            )\n",
                "        \n",
                "        # Determine number of windows\n",
                "        if pose_feat is not None:\n",
                "            n_windows = pose_feat.shape[0]\n",
                "        elif flow_feat is not None:\n",
                "            n_windows = flow_feat.shape[0]\n",
                "        else:\n",
                "            n_windows = 1\n",
                "        \n",
                "        # Return tensors (handle None cases)\n",
                "        if pose_feat is None:\n",
                "            pose_feat = np.zeros((n_windows, self.window_size, 16))\n",
                "        \n",
                "        if flow_feat is None:\n",
                "            flow_feat = np.zeros((n_windows, 3))\n",
                "        \n",
                "        # Align dimensions (use mean over window for flow alignment)\n",
                "        if len(pose_feat) != len(flow_feat):\n",
                "            min_len = min(len(pose_feat), len(flow_feat))\n",
                "            pose_feat = pose_feat[:min_len]\n",
                "            flow_feat = flow_feat[:min_len]\n",
                "        \n",
                "        return (\n",
                "            torch.tensor(pose_feat, dtype=torch.float32),\n",
                "            torch.tensor(flow_feat, dtype=torch.float32),\n",
                "            torch.tensor(label, dtype=torch.float32)\n",
                "        )\n",
                "\n",
                "\n",
                "print(\"âœ… CowLamenessDataset defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Transformer MIL Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 8.1 GOLD STANDARD MODEL (v25-v30 COMPLETE)\n",
                "# ============================================================\n",
                "\n",
                "class LamenessSeverityModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Complete Lameness Detection Model with:\n",
                "    - Multi-modal fusion (Pose + Flow + VideoMAE)\n",
                "    - Causal Transformer\n",
                "    - MIL Attention\n",
                "    - Severity Regression (0-3)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.config = config\n",
                "        \n",
                "        # Modal encoders\n",
                "        self.pose_encoder = nn.Sequential(\n",
                "            nn.Linear(config['POSE_DIM'], 128),\n",
                "            nn.ReLU(),\n",
                "            nn.LayerNorm(128)\n",
                "        ) if config['USE_POSE'] else None\n",
                "        \n",
                "        self.flow_encoder = nn.Sequential(\n",
                "            nn.Linear(config['FLOW_DIM'], 64),\n",
                "            nn.ReLU(),\n",
                "            nn.LayerNorm(64)\n",
                "        ) if config['USE_FLOW'] else None\n",
                "        \n",
                "        self.video_encoder = nn.Sequential(\n",
                "            nn.Linear(config['VIDEO_DIM'], 128),\n",
                "            nn.ReLU(),\n",
                "            nn.LayerNorm(128)\n",
                "        ) if config['USE_VIDEOMAE'] else None\n",
                "        \n",
                "        # Calculate total dimension\n",
                "        total_dim = 0\n",
                "        if config['USE_POSE']: total_dim += 128\n",
                "        if config['USE_FLOW']: total_dim += 64\n",
                "        if config['USE_VIDEOMAE']: total_dim += 128\n",
                "        self.total_dim = total_dim\n",
                "        \n",
                "        # Domain normalization (v29)\n",
                "        self.domain_norm = DomainNorm(total_dim)\n",
                "        \n",
                "        # Causal Transformer (v30)\n",
                "        self.temporal_encoder = CausalTransformerEncoder(\n",
                "            d_model=total_dim,\n",
                "            nhead=config['NUM_HEADS'],\n",
                "            num_layers=config['NUM_LAYERS']\n",
                "        )\n",
                "        \n",
                "        # MIL Attention\n",
                "        self.attention = nn.Sequential(\n",
                "            nn.Linear(total_dim, 64),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(64, 1)\n",
                "        )\n",
                "        \n",
                "        # Output head (regression or classification)\n",
                "        if config.get('MODE') == 'regression':\n",
                "            self.head = nn.Sequential(\n",
                "                nn.Linear(total_dim, 128),\n",
                "                nn.ReLU(),\n",
                "                nn.Dropout(0.3),\n",
                "                nn.Linear(128, 1),\n",
                "                nn.Sigmoid()  # Output [0, 1], scale to [0, 3]\n",
                "            )\n",
                "        else:\n",
                "            self.head = nn.Sequential(\n",
                "                nn.Linear(total_dim, 128),\n",
                "                nn.ReLU(),\n",
                "                nn.Dropout(0.3),\n",
                "                nn.Linear(128, 1),\n",
                "                nn.Sigmoid()\n",
                "            )\n",
                "    \n",
                "    def forward(self, pose, flow, video=None):\n",
                "        features = []\n",
                "        \n",
                "        if self.config['USE_POSE'] and self.pose_encoder is not None:\n",
                "            if pose.dim() == 4:\n",
                "                B, N, W, D = pose.shape\n",
                "                pose = pose.mean(dim=2)\n",
                "            features.append(self.pose_encoder(pose))\n",
                "        \n",
                "        if self.config['USE_FLOW'] and self.flow_encoder is not None:\n",
                "            features.append(self.flow_encoder(flow))\n",
                "        \n",
                "        if self.config['USE_VIDEOMAE'] and self.video_encoder is not None and video is not None:\n",
                "            features.append(self.video_encoder(video))\n",
                "        \n",
                "        x = torch.cat(features, dim=-1)\n",
                "        \n",
                "        # Domain normalization\n",
                "        x = self.domain_norm(x)\n",
                "        \n",
                "        # Causal transformer\n",
                "        h = self.temporal_encoder(x, use_causal=self.config.get('USE_CAUSAL', True))\n",
                "        \n",
                "        # MIL attention\n",
                "        attn_logits = self.attention(h).squeeze(-1)\n",
                "        attn_weights = F.softmax(attn_logits, dim=1)\n",
                "        bag = (h * attn_weights.unsqueeze(-1)).sum(dim=1)\n",
                "        \n",
                "        # Prediction\n",
                "        pred = self.head(bag).squeeze(-1)\n",
                "        \n",
                "        # Scale to [0, 3] for regression\n",
                "        if self.config.get('MODE') == 'regression':\n",
                "            pred = pred * 3.0\n",
                "        \n",
                "        return pred, attn_weights\n",
                "\n",
                "# Initialize model\n",
                "model = LamenessSeverityModel(CFG).to(DEVICE)\n",
                "\n",
                "print('âœ… LamenessSeverityModel initialized')\n",
                "print(f'   Mode: {CFG.get(\"MODE\", \"classification\")}')\n",
                "print(f'   Causal: {CFG.get(\"USE_CAUSAL\", True)}')\n",
                "print(f'   Total dimension: {model.total_dim}')\n",
                "print(f'   Parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 9.1 PREPARE DATA SPLITS\n",
                "# ============================================================\n",
                "\n",
                "# Build video list\n",
                "all_videos = []\n",
                "all_labels = []\n",
                "\n",
                "for v in healthy_videos:\n",
                "    all_videos.append(v)\n",
                "    all_labels.append(0)\n",
                "\n",
                "for v in lame_videos:\n",
                "    all_videos.append(v)\n",
                "    all_labels.append(1)\n",
                "\n",
                "# Train/test split\n",
                "train_videos, test_videos, train_labels, test_labels = train_test_split(\n",
                "    all_videos, all_labels,\n",
                "    test_size=0.2,\n",
                "    stratify=all_labels,\n",
                "    random_state=SEED\n",
                ")\n",
                "\n",
                "print(f\"ðŸ“Š Data Split:\")\n",
                "print(f\"   Train: {len(train_videos)} videos\")\n",
                "print(f\"   Test: {len(test_videos)} videos\")\n",
                "print(f\"   Train class balance: {sum(train_labels) / len(train_labels) * 100:.1f}% lame\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 9.2 CREATE DATALOADERS\n",
                "# ============================================================\n",
                "\n",
                "train_dataset = CowLamenessDataset(\n",
                "    train_videos, train_labels, CFG, POSE_DIR,\n",
                "    window_size=CFG[\"WINDOW_FRAMES\"], stride=CFG[\"STRIDE_FRAMES\"]\n",
                ")\n",
                "\n",
                "test_dataset = CowLamenessDataset(\n",
                "    test_videos, test_labels, CFG, POSE_DIR,\n",
                "    window_size=CFG[\"WINDOW_FRAMES\"], stride=CFG[\"STRIDE_FRAMES\"]\n",
                ")\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
                "\n",
                "print(f\"âœ… Dataloaders created\")\n",
                "print(f\"   Train batches: {len(train_loader)}\")\n",
                "print(f\"   Test batches: {len(test_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 9.3 GOLD STANDARD TRAINING (v26 REQUIREMENTS)\n",
                "# ============================================================\n",
                "\n",
                "# Loss function based on mode\n",
                "if CFG.get('MODE') == 'regression':\n",
                "    criterion = nn.MSELoss()\n",
                "    print('Using MSELoss for severity regression')\n",
                "else:\n",
                "    criterion = nn.BCELoss()\n",
                "    print('Using BCELoss for binary classification')\n",
                "\n",
                "# LR Groups (v26 requirement)\n",
                "param_groups = [\n",
                "    {'params': model.parameters(), 'lr': CFG['LR']}\n",
                "]\n",
                "\n",
                "if videomae_encoder is not None:\n",
                "    param_groups.append({\n",
                "        'params': videomae_encoder.parameters(), \n",
                "        'lr': CFG['LR_VIDEOMAE']\n",
                "    })\n",
                "    print(f'VideoMAE LR: {CFG[\"LR_VIDEOMAE\"]}')\n",
                "\n",
                "optimizer = optim.AdamW(param_groups, weight_decay=CFG['WEIGHT_DECAY'])\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "start_epoch = 0\n",
                "\n",
                "# Resume from checkpoint if exists (v26 requirement)\n",
                "checkpoint_path = f'{MODEL_DIR}/checkpoint.pt'\n",
                "if os.path.exists(checkpoint_path):\n",
                "    print(f'Loading checkpoint from {checkpoint_path}')\n",
                "    checkpoint = torch.load(checkpoint_path)\n",
                "    model.load_state_dict(checkpoint['model'])\n",
                "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
                "    start_epoch = checkpoint['epoch'] + 1\n",
                "    best_val_loss = checkpoint['best_val_loss']\n",
                "    train_losses = checkpoint.get('train_losses', [])\n",
                "    val_losses = checkpoint.get('val_losses', [])\n",
                "    print(f'Resuming from epoch {start_epoch}')\n",
                "\n",
                "print('\\n' + '='*60)\n",
                "print('TRAINING (Gold Standard v26+)')\n",
                "print('='*60)\n",
                "\n",
                "for epoch in range(start_epoch, CFG['EPOCHS']):\n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    \n",
                "    for pose, flow, label in train_loader:\n",
                "        pose = pose.to(DEVICE)\n",
                "        flow = flow.to(DEVICE)\n",
                "        label = label.to(DEVICE)\n",
                "        \n",
                "        # For regression, scale labels to [0, 3]\n",
                "        if CFG.get('MODE') == 'regression':\n",
                "            label = label * 3.0\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        pred, attn = model(pose, flow)\n",
                "        loss = criterion(pred, label)\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient clipping\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        \n",
                "        optimizer.step()\n",
                "        train_loss += loss.item()\n",
                "    \n",
                "    train_loss /= len(train_loader)\n",
                "    train_losses.append(train_loss)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for pose, flow, label in test_loader:\n",
                "            pose = pose.to(DEVICE)\n",
                "            flow = flow.to(DEVICE)\n",
                "            label = label.to(DEVICE)\n",
                "            \n",
                "            if CFG.get('MODE') == 'regression':\n",
                "                label = label * 3.0\n",
                "            \n",
                "            pred, attn = model(pose, flow)\n",
                "            loss = criterion(pred, label)\n",
                "            val_loss += loss.item()\n",
                "    \n",
                "    val_loss /= len(test_loader)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    # Save checkpoint (v26 requirement)\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save({\n",
                "            'model': model.state_dict(),\n",
                "            'optimizer': optimizer.state_dict(),\n",
                "            'epoch': epoch,\n",
                "            'best_val_loss': best_val_loss,\n",
                "            'train_losses': train_losses,\n",
                "            'val_losses': val_losses,\n",
                "            'config': CFG\n",
                "        }, f'{MODEL_DIR}/best_model.pt')\n",
                "    \n",
                "    # Save checkpoint for resume\n",
                "    torch.save({\n",
                "        'model': model.state_dict(),\n",
                "        'optimizer': optimizer.state_dict(),\n",
                "        'epoch': epoch,\n",
                "        'best_val_loss': best_val_loss,\n",
                "        'train_losses': train_losses,\n",
                "        'val_losses': val_losses,\n",
                "        'config': CFG\n",
                "    }, checkpoint_path)\n",
                "    \n",
                "    if (epoch + 1) % 5 == 0:\n",
                "        print(f'Epoch {epoch+1}/{CFG[\"EPOCHS\"]} | Train: {train_loss:.4f} | Val: {val_loss:.4f}')\n",
                "\n",
                "print('\\nâœ… Training complete')\n",
                "print(f'   Best validation loss: {best_val_loss:.4f}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 10.1 EVALUATION (v30 - SEVERITY METRICS)\n",
                "# ============================================================\n",
                "\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "\n",
                "# Load best model\n",
                "checkpoint = torch.load(f'{MODEL_DIR}/best_model.pt')\n",
                "model.load_state_dict(checkpoint['model'])\n",
                "model.eval()\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "all_attns = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for pose, flow, label in test_loader:\n",
                "        pose = pose.to(DEVICE)\n",
                "        flow = flow.to(DEVICE)\n",
                "        \n",
                "        pred, attn = model(pose, flow)\n",
                "        \n",
                "        all_preds.append(pred.cpu().numpy())\n",
                "        all_labels.append(label.numpy() * 3.0 if CFG.get('MODE') == 'regression' else label.numpy())\n",
                "        all_attns.append(attn.cpu().numpy())\n",
                "\n",
                "all_preds = np.concatenate(all_preds)\n",
                "all_labels = np.concatenate(all_labels)\n",
                "\n",
                "print('\\n' + '='*60)\n",
                "print('FINAL TEST RESULTS')\n",
                "print('='*60)\n",
                "\n",
                "if CFG.get('MODE') == 'regression':\n",
                "    mae = mean_absolute_error(all_labels, all_preds)\n",
                "    rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
                "    print(f'MAE (Mean Absolute Error): {mae:.3f}')\n",
                "    print(f'RMSE (Root Mean Squared Error): {rmse:.3f}')\n",
                "    print(f'Prediction range: [{all_preds.min():.2f}, {all_preds.max():.2f}]')\n",
                "else:\n",
                "    pred_binary = (all_preds > 0.5).astype(int)\n",
                "    accuracy = accuracy_score(all_labels, pred_binary)\n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, pred_binary, average='binary')\n",
                "    cm = confusion_matrix(all_labels, pred_binary)\n",
                "    \n",
                "    print(f'Accuracy:  {accuracy:.4f}')\n",
                "    print(f'Precision: {precision:.4f}')\n",
                "    print(f'Recall:    {recall:.4f}')\n",
                "    print(f'F1-Score:  {f1:.4f}')\n",
                "    print(f'\\nConfusion Matrix:')\n",
                "    print(cm)\n",
                "\n",
                "print('='*60)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 10.2 VISUALIZE RESULTS\n",
                "# ============================================================\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Training curve\n",
                "axes[0].plot(train_losses, label='Train')\n",
                "axes[0].plot(val_losses, label='Validation')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training Progress')\n",
                "axes[0].legend()\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Confusion matrix\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Healthy', 'Lame'],\n",
                "            yticklabels=['Healthy', 'Lame'], ax=axes[1])\n",
                "axes[1].set_xlabel('Predicted')\n",
                "axes[1].set_ylabel('True')\n",
                "axes[1].set_title(f'Confusion Matrix (Acc: {accuracy:.2%})')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{RESULT_DIR}/training_results.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nâœ… Results saved to {RESULT_DIR}/training_results.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Attention Visualization (Interpretability)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# 11.1 VISUALIZE TEMPORAL ATTENTION\n",
                "# ============================================================\n",
                "\n",
                "# Get a sample for visualization\n",
                "model.eval()\n",
                "sample_pose, sample_flow, sample_label = next(iter(test_loader))\n",
                "\n",
                "with torch.no_grad():\n",
                "    sample_pose = sample_pose.to(DEVICE)\n",
                "    sample_flow = sample_flow.to(DEVICE)\n",
                "    pred, attn = model(sample_pose, sample_flow)\n",
                "\n",
                "attn_weights = attn.cpu().numpy().squeeze()\n",
                "\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.bar(range(len(attn_weights)), attn_weights, color='steelblue')\n",
                "plt.xlabel('Window Index (Time)')\n",
                "plt.ylabel('Attention Weight')\n",
                "plt.title(f'Temporal Attention Weights\\nTrue: {\"Lame\" if sample_label.item() == 1 else \"Healthy\"} | Pred: {pred.item():.2f}')\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "# Highlight most attended windows\n",
                "top_k = 3\n",
                "top_indices = np.argsort(attn_weights)[-top_k:]\n",
                "for idx in top_indices:\n",
                "    plt.bar(idx, attn_weights[idx], color='red', alpha=0.7)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{RESULT_DIR}/attention_visualization.png\", dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nðŸ“Š Top {top_k} attended windows: {top_indices}\")\n",
                "print(f\"   These windows may contain lameness indicators\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## âœ… Pipeline Complete\n",
                "\n",
                "**Summary:**\n",
                "- Biomechanical pose features extracted (16 features)\n",
                "- Optical flow motion analysis\n",
                "- Transformer MIL for weak-label learning\n",
                "- Interpretable attention weights\n",
                "\n",
                "**Next Steps:**\n",
                "1. Enable VideoMAE for improved performance (`CFG[\"USE_VIDEOMAE\"] = True`)\n",
                "2. Run ablation studies (Pose only, Flow only, etc.)\n",
                "3. Tune hyperparameters (window size, learning rate)\n",
                "4. Add severity regression (0-3 score)\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}